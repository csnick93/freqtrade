---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import json
import numpy as np
import pandas as pd
import os
from pathlib import Path
from typing import Tuple
from tqdm.notebook import tqdm
import sklearn
import plotly.graph_objects as go
```

```{python}
import talib.abstract as ta
from technical import indicators as ftt
```

# Read in data routines

```{python}
root_dir = Path('/Users/nicolasvonroden/Data/freqtrade/data_06_01/binance')
```

```{python}
ticker_file = root_dir / os.listdir(root_dir)[0]
```

```{python}
ZEMA_LEN_BUY = 72
ZEMA_LEN_SELL = 51
HIGH_OFFSET = 1.004
LOW_OFFSET = 0.964
```

```{python}
def ssl_atr(dataframe, length=7):
    df = dataframe.copy()
    df['smaHigh'] = df['high'].rolling(length).mean() + df['atr']
    df['smaLow'] = df['low'].rolling(length).mean() - df['atr']
    df['hlv'] = np.where(df['close'] > df['smaHigh'], 1,
                         np.where(df['close'] < df['smaLow'], -1, np.NAN))
    df['hlv'] = df['hlv'].ffill()
    df['sslDown'] = np.where(df['hlv'] < 0, df['smaHigh'], df['smaLow'])
    df['sslUp'] = np.where(df['hlv'] < 0, df['smaLow'], df['smaHigh'])
    return df['sslDown'], df['sslUp']
```

```{python}
def slow_tf_indicators(dataframe):
    displacement = 30
    ichimoku = ftt.ichimoku(dataframe,
                            conversion_line_period=20,
                            base_line_periods=60, 
                            laggin_span=120,       
                            displacement=displacement)

    dataframe['chikou_span'] = ichimoku['chikou_span']

    # cross indicators
    dataframe['tenkan_sen'] = ichimoku['tenkan_sen']
    dataframe['kijun_sen'] = ichimoku['kijun_sen']

    # cloud, green a > b, red a < b
    dataframe['senkou_a'] = ichimoku['senkou_span_a']
    dataframe['senkou_b'] = ichimoku['senkou_span_b']
    dataframe['leading_senkou_span_a'] = ichimoku['leading_senkou_span_a']
    dataframe['leading_senkou_span_b'] = ichimoku['leading_senkou_span_b']
    dataframe['cloud_green'] = ichimoku['cloud_green'] * 1
    dataframe['cloud_red'] = ichimoku['cloud_red'] * -1

    dataframe.loc[:, 'cloud_top'] = dataframe.loc[:,
                                                  ['senkou_a', 'senkou_b'
                                                   ]].max(axis=1)
    dataframe.loc[:,
                  'cloud_bottom'] = dataframe.loc[:,
                                                  ['senkou_a', 'senkou_b'
                                                   ]].min(axis=1)

    # DANGER ZONE START

    # NOTE: Not actually the future, present data that is normally shifted forward for display as the cloud
    dataframe['future_green'] = (
        dataframe['leading_senkou_span_a'] >
        dataframe['leading_senkou_span_b']).astype('int') * 2
    dataframe['future_red'] = (
        dataframe['leading_senkou_span_a'] <
        dataframe['leading_senkou_span_b']).astype('int') * 2 

    # The chikou_span is shifted into the past, so we need to be careful not to read the
    # current value.  But if we shift it forward again by displacement it should be safe to use.
    # We're effectively "looking back" at where it normally appears on the chart.
    dataframe['chikou_high'] = (
        (dataframe['chikou_span'] > dataframe['cloud_top']
         )).shift(displacement).fillna(0).astype('int') 

    dataframe['chikou_low'] = (
        (dataframe['chikou_span'] < dataframe['cloud_bottom']
         )).shift(displacement).fillna(0).astype('int')

    # DANGER ZONE END

    dataframe['atr'] = ta.ATR(dataframe, timeperiod=14)
    ssl_down, ssl_up = ssl_atr(dataframe, 10)
    dataframe['ssl_down'] = ssl_down
    dataframe['ssl_up'] = ssl_up
    dataframe['ssl_ok'] = ((ssl_up > ssl_down)).astype('int') * 3
    dataframe['ssl_bear'] = ((ssl_up < ssl_down)).astype('int') * 3

    dataframe['ichimoku_ok'] = (
        (dataframe['tenkan_sen'] > dataframe['kijun_sen'])
        & (dataframe['close'] > dataframe['cloud_top'])
        & (dataframe['future_green'] > 0)
        & (dataframe['chikou_high'] > 0)).astype('int') * 4

    dataframe['ichimoku_bear'] = (
        (dataframe['tenkan_sen'] < dataframe['kijun_sen'])
        & (dataframe['close'] < dataframe['cloud_bottom'])
        & (dataframe['future_red'] > 0)
        & (dataframe['chikou_low'] > 0)).astype('int') * 4

    dataframe['ichimoku_valid'] = (
        (dataframe['leading_senkou_span_b']
         == dataframe['leading_senkou_span_b'])  # not NaN
    ).astype('int') * 1

    dataframe['trend_pulse'] = (
        (dataframe['ichimoku_ok'] > 0)
        & (dataframe['ssl_ok'] > 0)).astype('int') * 2

    dataframe['bear_trend_pulse'] = (
        (dataframe['ichimoku_bear'] > 0)
        & (dataframe['ssl_bear'] > 0)).astype('int') * 2

    dataframe['trend_over'] = (
        (dataframe['ssl_ok'] == 0)
        | (dataframe['close'] < dataframe['cloud_top'])).astype('int') * 1

    dataframe['bear_trend_over'] = (
        (dataframe['ssl_bear'] == 0)
        |
        (dataframe['close'] > dataframe['cloud_bottom'])).astype('int') * 1

    dataframe.loc[(dataframe['trend_pulse'] > 0), 'trending'] = 3
    dataframe.loc[(dataframe['trend_over'] > 0), 'trending'] = 0
    dataframe['trending'].fillna(method='ffill', inplace=True)

    dataframe.loc[(dataframe['bear_trend_pulse'] > 0), 'bear_trending'] = 3
    dataframe.loc[(dataframe['bear_trend_over'] > 0), 'bear_trending'] = 0
    dataframe['bear_trending'].fillna(method='ffill', inplace=True)

    return dataframe

```

```{python}
def fast_tf_indicators(dataframe):
    dataframe[f'zema_{ZEMA_LEN_BUY}'] = ftt.zema(
                dataframe, period=ZEMA_LEN_BUY)
    dataframe[f'zema_{ZEMA_LEN_SELL}'] = ftt.zema(
        dataframe, period=ZEMA_LEN_SELL)
    dataframe['zema_buy'] = ftt.zema(
        dataframe,
        period=ZEMA_LEN_BUY) * LOW_OFFSET
    dataframe['zema_sell'] = ftt.zema(
        dataframe,
        period=ZEMA_LEN_SELL) * HIGH_OFFSET

    return dataframe
```

```{python}
def populate_indicators(dataframe):
    dataframe = fast_tf_indicators(dataframe)
    dataframe = slow_tf_indicators(dataframe)

    return dataframe
```

```{python}
def read_ticker_data(ticker_file):
    content = pd.read_json(ticker_file.read_text())
    content.columns = ['time', 'close', 'high', 'low', 'open', 'volume']
    
    indicator_df = populate_indicators(content)
    
    indicator_df = indicator_df.dropna()
    indicator_df = (indicator_df - indicator_df.min()) / (indicator_df.max() - indicator_df.min())
    
    return indicator_df
```

# Generate data samples


* Take time series of length 500 (500 * 5 min ~ 42 hours), because that's what obelisk strategy is using right now
* Leave around 1 hour in between samples (i.e. 12 timesteps)
* Label is determined whether after purchase decision the price increases by 2% within 2 hours
* Separate into train, val and test set by 
    * coin
    * time period (separate by 30 day intervals as dataset is made up of 90 days)
    * make sure label distribution is somewhat stratified amongst folds

```{python}
target_dir = Path('/Users/nicolasvonroden/Data/freqtrade/prediction_data')
target_dir.mkdir(exist_ok=True)
```

```{python}
SAMPLE_LENGTH = 24*12
SAMPLE_STEPSIZE = 12
FORECAST_LENGTH = 24
INCREASE_TARGET = 0.05
```

```{python}
def compute_label(current_price, highest_future_price) -> int:
    perc_change = (highest_future_price - current_price) / current_price    
    if (perc_change >= INCREASE_TARGET):
        label = 1
    else:
        label = 0
    return label
```

```{python}
def create_samples(ticker_df) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    samples = []
    labels = []
    for i in range(int((ticker_df.shape[0] - 2*SAMPLE_LENGTH)/ SAMPLE_STEPSIZE)):
        sample = ticker_df.iloc[i * SAMPLE_STEPSIZE : 
                                    i * SAMPLE_STEPSIZE + SAMPLE_LENGTH]
        
        future_prices = ticker_df.close.iloc[i * SAMPLE_STEPSIZE + SAMPLE_LENGTH : 
                                          i * SAMPLE_STEPSIZE + SAMPLE_LENGTH + FORECAST_LENGTH]
        label = compute_label(sample.close.iloc[-1], np.max(future_prices))
        
        samples.append(sample[['close', 'high', 'low', 'open', 'volume', 'zema_72', 'zema_51',
                               'zema_buy', 'zema_sell', 'chikou_span', 'tenkan_sen', 'kijun_sen',
                               'senkou_a', 'senkou_b', 'leading_senkou_span_a',
                               'leading_senkou_span_b', 'cloud_green', 'cloud_red', 'cloud_top',
                               'cloud_bottom', 'future_green', 'future_red', 'chikou_high',
                               'chikou_low', 'atr', 'ssl_down', 'ssl_up', 'ssl_ok', 'ssl_bear',
                               'ichimoku_ok', 'ichimoku_bear', 'ichimoku_valid', 'trend_pulse',
                               'bear_trend_pulse', 'trend_over', 'bear_trend_over', 'trending',
                               'bear_trending']].values)
        labels.append(label)
        
    return np.asarray(samples), np.asarray(labels)
```

## Double check correctness

```{python}
ticker_file = root_dir / os.listdir(root_dir)[7]
ticker_df = read_ticker_data(ticker_file)
```

```{python}
samples, labels = create_samples(ticker_df)
```

## Create data files

```{python}
for ticker_file in tqdm(os.listdir(root_dir)):
    try:
        ticker_df = read_ticker_data(root_dir / ticker_file)
        samples, labels = create_samples(ticker_df)
        ticker_name = (root_dir / ticker_file).stem
        np.savez(target_dir / f'{ticker_name}.npz', X=samples, y=labels)
    except Exception as e:
        print(f'Issue with {ticker_file}: {e}')
        continue
```

# Train a classifier on the data


* For now, simply separate folds by coins

```{python}
from sklearn import neural_network
```

```{python}
data_files = os.listdir(target_dir)
```

```{python}
num_train = int(len(data_files) * 0.7)
num_val = int(len(data_files) * 0.1)
num_test = int(len(data_files) * 0.2)

train_files = data_files[:num_train]
val_files = data_files[num_train : num_train+num_val]
test_files = data_files[num_train+num_val:]
```

```{python}
X_train = np.concatenate([np.load(target_dir / f)['X'] for f in train_files + val_files])
#X_val = np.concatenate([np.load(target_dir / f)['X'] for f in val_files])
X_test = np.concatenate([np.load(target_dir / f)['X'] for f in test_files])

y_train = np.concatenate([np.load(target_dir / f)['y'] for f in train_files + val_files])
#y_val = np.concatenate([np.load(target_dir / f)['y'] for f in val_files])
y_test = np.concatenate([np.load(target_dir / f)['y'] for f in test_files])
```

## Plot data

```{python}
X_train_pos = X_train[y_train==1]
```

```{python}
X_train_pos.shape
```

```{python}
train_mean = X_train_pos.mean(axis=0)
train_std = X_train_pos.std(axis=0)
```

```{python}
fig = go.Figure()
c = 'blue'
fig.add_trace(go.Scatter(x=np.arange(4), y=train_mean+train_std,
                                     mode='lines',
                                     line=dict(color=c,width =0.1),
                                     name='upper bound'))
fig.add_trace(go.Scatter(x=np.arange(4), y=train_mean,
                         mode='lines',
                         line=dict(color=c),
                         fill='tonexty',
                         name='mean'))
fig.add_trace(go.Scatter(x=np.arange(4), y=train_mean-train_std,
                         mode='lines',
                         line=dict(color=c, width =0.1),
                         fill='tonexty',
                         name='lower bound'))
fig.show()
```

## Train model

```{python}
mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='logistic', solver='adam',
         learning_rate_init = 1e-4, early_stopping=True, validation_fraction = 0.2, n_iter_no_change=20,
         random_state=42)
```

```{python}
clf = mlp.fit(X_train, y_train)
```

```{python}
clf.score(X_train, y_train)
```

```{python}
clf.score(X_test, y_test)
```

```{python}
y_test[:10]
```

```{python}
clf.predict(X_test[:10])
```

```{python}

```
